{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ei_0Kk37F174"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import string\n",
        "from functools import reduce\n",
        "from math import log\n",
        "import itertools\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter smoothing or no smoothing.\n",
        "smoothing = 1"
      ],
      "metadata": {
        "id": "K5X7VC7QF913"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(filename):\n",
        "  dataset = []\n",
        "  with open(filename, 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "      lyrics = row.get('Lyrics', '')\n",
        "      dataset.append(lyrics)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "6LKOyi5SGAB-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentence(lines):\n",
        "    tokenized_sentences = [re.findall(r'\\b\\w+\\b', sentence.lower()) for sentence in lines]\n",
        "    return tokenized_sentences"
      ],
      "metadata": {
        "id": "YXJtCeo4GEhG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data(lines):\n",
        "    processed_data = []\n",
        "    for sentence in lines:\n",
        "        # Remove punctuations\n",
        "        sentence = [word for word in sentence if word not in string.punctuation]\n",
        "        # Remove empty strings\n",
        "        sentence = [word for word in sentence if word]\n",
        "        # Lowercase all the words\n",
        "        sentence = [word.lower() for word in sentence]\n",
        "        # Add <s> at the beginning and </s> at the end of every sentence\n",
        "        sentence = ['<s>'] + sentence + ['</s>']\n",
        "        processed_data.append(sentence)\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "LlfG9ihTGGRB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_file(\"/content/songs.csv\")\n",
        "dataset = tokenize_sentence(dataset)\n",
        "dataset = prep_data(dataset)"
      ],
      "metadata": {
        "id": "85OxKojBGIzz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "\n",
        "No of sentences in Corpus: 10059\n",
        "\n",
        "No of sentences in Corpus: 10059\n",
        "\n",
        "No of sentences in Corpus: 10059\n"
      ],
      "metadata": {
        "id": "QKZDS7RSVP9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the vocabulary file of the dataset.\n",
        "def vocabulary(dataset):\n",
        "    dataset_vocab = set(itertools.chain.from_iterable(dataset))\n",
        "    # remove <s> and </s> from the vocabulary of the dataset\n",
        "    dataset_vocab.remove('<s>')\n",
        "    dataset_vocab.remove('</s>')\n",
        "    dataset_vocab = list(dataset_vocab)\n",
        "    dataset_vocab.append('<s>')\n",
        "    dataset_vocab.append('</s>')\n",
        "    return dataset_vocab\n",
        "\n",
        "dataset_vocab = vocabulary(dataset)"
      ],
      "metadata": {
        "id": "ZbDuYgtQGLHv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_vocab)"
      ],
      "metadata": {
        "id": "K565y1RMGOTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48542bb8-62db-44f9-960e-d6dfabddbbff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48928"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_of_unique_words(lines):\n",
        "    bag_of_words = list(itertools.chain.from_iterable(lines))  # change the nested list to one single list\n",
        "    word_count = len(bag_of_words) - 2 # No of words in the corpus excluding <s> and </s>.\n",
        "    #count the no. of times a word repeats\n",
        "    count = {}\n",
        "\n",
        "    for word in bag_of_words:\n",
        "        if word not in count:\n",
        "            count[word] = 1\n",
        "        else:\n",
        "            count[word] += 1\n",
        "\n",
        "\n",
        "    # Number of unique words in the corpus excluding <s> and </s>\n",
        "    unique_word_count = len(set(bag_of_words)) - 2  # Subtract 2 for <s> and </s>\n",
        "\n",
        "    #print(\"!!! IT IS EXCLUDING <s> AND </s> !!!\")\n",
        "    print(\"No of unique words : \"+ str(unique_word_count))\n",
        "    print(\"No of words : \"+ str(word_count))\n",
        "\n",
        "    return count"
      ],
      "metadata": {
        "id": "7cDBUhYwGQQf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_word_frequency = freq_of_unique_words(dataset)\n",
        "len(unique_word_frequency)"
      ],
      "metadata": {
        "id": "Qp0HTK3BGT1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4adf0f-cdfd-4235-b340-8a5c5eab386f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of unique words : 48926\n",
            "No of words : 382097\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48928"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Expected Output: `**\n",
        "\n",
        "No of unique words in corpus : 17139\n",
        "No of words in corpus: 218619"
      ],
      "metadata": {
        "id": "xGcBePnKS1dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bigram_frequencies(lines):\n",
        "    bigram_frequencies = dict()\n",
        "    #unique_bigrams = set()\n",
        "\n",
        "    for sentence in lines:\n",
        "        for i in range(len(sentence) - 1):\n",
        "            current_word = sentence[i]\n",
        "            next_word = sentence[i + 1]\n",
        "            bigram = (current_word, next_word)\n",
        "\n",
        "            if bigram not in bigram_frequencies:\n",
        "                bigram_frequencies[bigram] = 1\n",
        "            else:\n",
        "                bigram_frequencies[bigram] += 1\n",
        "\n",
        "    #The number of bigram_frquencies\n",
        "    #print(len(bigram_frequencies))\n",
        "    return bigram_frequencies\n"
      ],
      "metadata": {
        "id": "wvS7xf6bGWEy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_frequencies = compute_bigram_frequencies(dataset)\n",
        "#print(bigram_frequencies)\n",
        "bigram_unique_word_count = len(unique_word_frequency)\n",
        "print(\"\\n\"+\"No of words in bigram: \"+str(bigram_unique_word_count))"
      ],
      "metadata": {
        "id": "R7RVeWKDGYYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0688b45a-fac5-4244-fc33-5fd7fb329630"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No of words in bigram: 48928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bigram_probabilities(bigram_frequencies, count):\n",
        "    bigram_probabilities = dict()\n",
        "    for key in bigram_frequencies:\n",
        "        numerator = bigram_frequencies[key]\n",
        "        denominator = count.get(key[0], 0)\n",
        "        if (numerator ==0 or denominator==0):\n",
        "            bigram_probabilities[key] = 0.0\n",
        "        else:\n",
        "            bigram_probabilities[key] = numerator / denominator\n",
        "    return bigram_probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "5gRKLtoWGai4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_probabilities = compute_bigram_probabilities(bigram_frequencies,unique_word_frequency)\n"
      ],
      "metadata": {
        "id": "sfQZmmxXGdfg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bigram_count_test_sentence(given_word,word,smoothing):\n",
        "    if smoothing==0:\n",
        "        return 0 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))\n",
        "    elif smoothing == 1:\n",
        "        return 1 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))+1"
      ],
      "metadata": {
        "id": "Kpog7IjFK9fs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A table showing the bigram counts for test sentence.\n",
        "def print_bigram_freq_test_sentence(test_sentence_vocab,smoothing):\n",
        "    print(\"A table showing the bigram counts for test sentence.\"+\"\\nsmoothing =\"+str(smoothing))\n",
        "    print(\"\\t\\t\\t\", end=\"\")\n",
        "    for word in test_sentence_vocab:\n",
        "        if word != '<s>':\n",
        "            print(word, end=\"\\t\\t\")\n",
        "    print(\"\")\n",
        "    for given_word in test_sentence_vocab:\n",
        "        if given_word != '</s>':\n",
        "            if(smoothing==1):\n",
        "                print(unique_word_frequency.get(given_word)+bigram_unique_word_count, end =\"\\t\")\n",
        "            elif(smoothing==0):\n",
        "                print(unique_word_frequency.get(given_word), end =\"\\t\")\n",
        "            print(given_word, end=\"\\t\\t\")\n",
        "            for word in test_sentence_vocab:\n",
        "                if word !='<s>':\n",
        "                    print(\"{0:}\".format(compute_bigram_count_test_sentence(given_word,word,smoothing)), end=\"\\t\\t\")\n",
        "            print(\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "YnBpCW7sLFNb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram probabilities of the test sentence computed using the bigram probabilities of the training data.\n",
        "# add-one smoothing if 1, no smoothing if 0 ---- smoothing\n",
        "def compute_bigram_prob_test_sentence(given_word,word,smoothing):\n",
        "    bigram_freq = 0 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))\n",
        "    uni_freq = 0 if unique_word_frequency.get((given_word))==None else unique_word_frequency.get((given_word))\n",
        "    if smoothing==0:\n",
        "        return 0 if bigram_probabilities.get((given_word,word))==None else bigram_probabilities.get((given_word,word))\n",
        "    elif smoothing == 1:\n",
        "        numerator = bigram_freq+1\n",
        "        denominator = uni_freq+bigram_unique_word_count\n",
        "        return 0.0 if numerator == 0 or denominator == 0 else float(numerator) / float(denominator)"
      ],
      "metadata": {
        "id": "3bv-I2N2LJ28"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A table showing the bigram probabilities for test sentence.\n",
        "def print_bigram_probabilities_test_sentence(test_sentence_vocab,smoothing):\n",
        "    print(\"A table showing the bigram probabilities for test sentence\"+\"\\nsmoothing =\"+str(smoothing))\n",
        "    print(\"\\t\\t\", end=\"\")\n",
        "    for word in test_sentence_vocab:\n",
        "        if word != '<s>':\n",
        "            print(word, end=\"\\t\\t\")\n",
        "    print(\"\")\n",
        "    for given_word in test_sentence_vocab:\n",
        "        if given_word != '</s>':\n",
        "            print(given_word, end=\"\\t\\t\")\n",
        "            for word in test_sentence_vocab:\n",
        "                if word !='<s>':\n",
        "                    print(\"{0:.5f}\".format(compute_bigram_prob_test_sentence(given_word,word,smoothing)), end=\"\\t\\t\")\n",
        "            print(\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "niKUrR5MLQIA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the probability of the test sentence\n",
        "# for add-one smoothing if 1, no smoothing if 0\n",
        "def compute_prob_test_sentence(sentence,smoothing):\n",
        "    test_sent_prob = 0\n",
        "\n",
        "    if(smoothing == 0):\n",
        "        given_word = None\n",
        "        for word in sentence:\n",
        "            if given_word!=None:\n",
        "                if bigram_probabilities.get((given_word,word))==0 or bigram_probabilities.get((given_word,word))== None:\n",
        "                    return 0\n",
        "                else:\n",
        "                    test_sent_prob+=log((bigram_probabilities.get((given_word,word),0)),10)\n",
        "            given_word = word\n",
        "\n",
        "    elif(smoothing ==1):\n",
        "        given_word = None\n",
        "        for word in sentence:\n",
        "            if given_word!=None:\n",
        "                bigram_freq = 0 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))\n",
        "                uni_freq = 0 if unique_word_frequency.get((given_word))==None else unique_word_frequency.get((given_word))\n",
        "                numerator = bigram_freq+1\n",
        "                denominator = uni_freq+bigram_unique_word_count\n",
        "                probability = 0 if numerator==0 or denominator ==0 else float(numerator)/float(denominator)\n",
        "                if(probability==0):\n",
        "                    return 0\n",
        "                test_sent_prob +=log(probability,10)\n",
        "            given_word = word\n",
        "\n",
        "    return 10**test_sent_prob\n"
      ],
      "metadata": {
        "id": "k7MoPugcLUEF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test sentence here\n",
        "test_sentences = [['بايع و شاري رابح في خطاري مركيت نهاري تعرف اخباري'],['و باقي نجيبو المال نعيش معاهم رايض أما عندي ناموسي حرب مخّي خايض']]"
      ],
      "metadata": {
        "id": "zXpen81yLa9z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(test_sentences)):\n",
        "    test_sentence = test_sentences[i]\n",
        "    print(\"!!!!!!!!!!The test Sentence is!!!!!!!!!!\")\n",
        "    print(test_sentence)\n",
        "    test_sentence = tokenize_sentence(test_sentence)\n",
        "    test_sentence = prep_data(test_sentence)\n",
        "\n",
        "    # Vocabulary of test sentence\n",
        "    test_sentence_vocab = vocabulary(test_sentence)\n",
        "\n",
        "    test_sentence = list(itertools.chain.from_iterable(test_sentence))\n",
        "    #test_sentence\n",
        "\n",
        "    # A table showing the bigram counts for test sentence.\n",
        "    print_bigram_freq_test_sentence(test_sentence_vocab,smoothing)\n",
        "\n",
        "    # A table showing the bigram probabilities for test sentence.\n",
        "    print_bigram_probabilities_test_sentence(test_sentence_vocab,smoothing)\n",
        "\n",
        "    # The probability of the sentence under the trained model\n",
        "    print(\"The probability of the sentence under the trained model\"+\"\\nsmoothing =\"+str(smoothing))\n",
        "    print(compute_prob_test_sentence(test_sentence,0))"
      ],
      "metadata": {
        "id": "mNZWYXkRLgy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a462b9-b4c3-46d7-e7eb-4823d7398ebe"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!!!The test Sentence is!!!!!!!!!!\n",
            "['بايع و شاري رابح في خطاري مركيت نهاري تعرف اخباري']\n",
            "A table showing the bigram counts for test sentence.\n",
            "smoothing =1\n",
            "\t\t\tاخباري\t\tخطاري\t\tو\t\tشاري\t\tمركيت\t\tفي\t\tبايع\t\tتعرف\t\tنهاري\t\tرابح\t\t</s>\t\t\n",
            "48932\tاخباري\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48932\tخطاري\t\t1\t\t1\t\t1\t\t1\t\t5\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "50300\tو\t\t1\t\t1\t\t4\t\t6\t\t1\t\t10\t\t1\t\t1\t\t1\t\t1\t\t3\t\t\n",
            "48937\tشاري\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t5\t\t1\t\t\n",
            "48932\tمركيت\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t5\t\t1\t\t1\t\t\n",
            "49746\tفي\t\t1\t\t5\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48932\tبايع\t\t1\t\t1\t\t5\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48948\tتعرف\t\t5\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48934\tنهاري\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t5\t\t1\t\t1\t\t1\t\t\n",
            "48939\tرابح\t\t1\t\t1\t\t2\t\t1\t\t1\t\t5\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "49920\t<s>\t\t1\t\t1\t\t2\t\t1\t\t1\t\t4\t\t3\t\t1\t\t1\t\t1\t\t2\t\t\n",
            "\n",
            "A table showing the bigram probabilities for test sentence\n",
            "smoothing =1\n",
            "\t\tاخباري\t\tخطاري\t\tو\t\tشاري\t\tمركيت\t\tفي\t\tبايع\t\tتعرف\t\tنهاري\t\tرابح\t\t</s>\t\t\n",
            "اخباري\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "خطاري\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00010\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "و\t\t0.00002\t\t0.00002\t\t0.00008\t\t0.00012\t\t0.00002\t\t0.00020\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t\n",
            "شاري\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00010\t\t0.00002\t\t\n",
            "مركيت\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00010\t\t0.00002\t\t0.00002\t\t\n",
            "في\t\t0.00002\t\t0.00010\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "بايع\t\t0.00002\t\t0.00002\t\t0.00010\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "تعرف\t\t0.00010\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "نهاري\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00010\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "رابح\t\t0.00002\t\t0.00002\t\t0.00004\t\t0.00002\t\t0.00002\t\t0.00010\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "<s>\t\t0.00002\t\t0.00002\t\t0.00004\t\t0.00002\t\t0.00002\t\t0.00008\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00004\t\t\n",
            "\n",
            "The probability of the sentence under the trained model\n",
            "smoothing =1\n",
            "0\n",
            "!!!!!!!!!!The test Sentence is!!!!!!!!!!\n",
            "['و باقي نجيبو المال نعيش معاهم رايض أما عندي ناموسي حرب مخّي خايض']\n",
            "A table showing the bigram counts for test sentence.\n",
            "smoothing =1\n",
            "\t\t\tنعيش\t\tي\t\tمعاهم\t\tباقي\t\tحرب\t\tعندي\t\tمخ\t\tالمال\t\tو\t\tناموسي\t\tأما\t\tخايض\t\tرايض\t\tنجيبو\t\t</s>\t\t\n",
            "49005\tنعيش\t\t1\t\t1\t\t7\t\t1\t\t1\t\t1\t\t1\t\t1\t\t5\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "49086\tي\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t9\t\t1\t\t1\t\t3\t\t1\t\t1\t\t2\t\t\n",
            "48941\tمعاهم\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t\n",
            "48972\tباقي\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t\n",
            "48930\tحرب\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48989\tعندي\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48933\tمخ\t\t1\t\t4\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48934\tالمال\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "50300\tو\t\t5\t\t2\t\t1\t\t11\t\t1\t\t6\t\t2\t\t1\t\t4\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t\n",
            "48930\tناموسي\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48965\tأما\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48930\tخايض\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48930\tرايض\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "48932\tنجيبو\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t3\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t1\t\t\n",
            "49920\t<s>\t\t1\t\t1\t\t1\t\t1\t\t1\t\t2\t\t1\t\t1\t\t2\t\t1\t\t1\t\t1\t\t1\t\t1\t\t2\t\t\n",
            "\n",
            "A table showing the bigram probabilities for test sentence\n",
            "smoothing =1\n",
            "\t\tنعيش\t\tي\t\tمعاهم\t\tباقي\t\tحرب\t\tعندي\t\tمخ\t\tالمال\t\tو\t\tناموسي\t\tأما\t\tخايض\t\tرايض\t\tنجيبو\t\t</s>\t\t\n",
            "نعيش\t\t0.00002\t\t0.00002\t\t0.00014\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00010\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "ي\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00018\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00004\t\t\n",
            "معاهم\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t\n",
            "باقي\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t\n",
            "حرب\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "عندي\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "مخ\t\t0.00002\t\t0.00008\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "المال\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "و\t\t0.00010\t\t0.00004\t\t0.00002\t\t0.00022\t\t0.00002\t\t0.00012\t\t0.00004\t\t0.00002\t\t0.00008\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t\n",
            "ناموسي\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "أما\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "خايض\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "رايض\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "نجيبو\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00006\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t\n",
            "<s>\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00004\t\t0.00002\t\t0.00002\t\t0.00004\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00002\t\t0.00004\t\t\n",
            "\n",
            "The probability of the sentence under the trained model\n",
            "smoothing =1\n",
            "0\n"
          ]
        }
      ]
    }
  ]
}